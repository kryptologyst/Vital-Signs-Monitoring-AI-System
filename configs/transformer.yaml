# Transformer model configuration
defaults:
  - default

model:
  name: "transformer"
  params:
    d_model: 128
    nhead: 8
    num_layers: 4
    dropout_rate: 0.1
    max_seq_length: 1000

training:
  learning_rate: 0.0005
  batch_size: 8
  weight_decay: 1e-5
